{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script trains and optimize machine learning models (e.g., Lasso and Histogram-based Gradient Boosting) on climate datasets for annual predictions. It loads experiment configurations from a YAML file, prepares training and validation datasets (including optional CMIP6 scenarios and ERA5 data), defines model search spaces for hyperparameter tuning, and performs model training and evaluation. The trained models and configurations are saved to disk for later use or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from warnings import warn\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "sys.path.append('/home/vgarcia/notebooks')\n",
    "\n",
    "from models_functions import *\n",
    "from experiments_functions import *\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed invalid dataset: ssp585_mri-esm2-0\n",
      "...TRAINING ARGUMENTS...\n",
      "test_mode: false\n",
      "overwrite_experiment: true\n",
      "experiment_name: Optimize_ML_HGBR\n",
      "models:\n",
      "- HGBR\n",
      "train_val_YearsGap: 5\n",
      "n_jobs: 6\n",
      "use_era5: true\n",
      "cmip6_models:\n",
      "- access-cm2\n",
      "- cmcc-esm2\n",
      "- inm-cm4-8\n",
      "- inm-cm5-0\n",
      "- miroc-es2l\n",
      "- mpi-esm1-2-lr\n",
      "- mri-esm2-0\n",
      "- noresm2-mm\n",
      "scenarios:\n",
      "- historical\n",
      "- ssp126\n",
      "- ssp245\n",
      "- ssp585\n",
      "\n",
      "........................\n"
     ]
    }
   ],
   "source": [
    "# Load YAML from a file\n",
    "with open('/home/vgarcia/ML_annual/config_trainML.yml', 'r') as file:\n",
    "    args = yaml.safe_load(file)\n",
    "\n",
    "# check inputs\n",
    "preprocessing_path = \"/data/dl20-data/climate_operational/Victor_data/preprocessed_datasets_ML_new\"\n",
    "\n",
    "available_models= {\"Lasso\" : Lasso(random_state=123),\n",
    "              \"HGBR\": HistGradientBoostingRegressor(\n",
    "    random_state=123,\n",
    "    warm_start=True,\n",
    "    validation_fraction=0.3,\n",
    "    verbose=1)}\n",
    "\n",
    "# Define models\n",
    "search_spaces = {\n",
    "    \"HGBR\": {\n",
    "        'estimator__min_samples_leaf': Integer(1, 50),\n",
    "        'estimator__max_features': Real(0.3, 1.0, prior='uniform'),\n",
    "    },\n",
    "    \"Lasso\" : {\n",
    "        'estimator__alpha': Real(1e-5, 10.0, prior=\"log-uniform\"),\n",
    "        \"estimator__tol\" : Real(1e-5, 1e-2, prior=\"log-uniform\"),\n",
    "        \"estimator__max_iter\": Integer(500, 5000, prior=\"uniform\")\n",
    "}}\n",
    "\n",
    "out_dir = f\"/home/vgarcia/experiments/ML_annual_new/{args['experiment_name']}/\"\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=args[\"overwrite_experiment\"])\n",
    "\n",
    "models_dict = {}\n",
    "# ensure models and parameters exist\n",
    "for model in args['models']:\n",
    "    if model not in available_models:\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        models_dict[model] = available_models[model]\n",
    "\n",
    "# list all datasets to process\n",
    "if \"cmip6\" and \"scenarios\" in args:\n",
    "    datasets = [\n",
    "        f\"{scenario}_{model}\"\n",
    "        for model in args[\"cmip6_models\"]\n",
    "        for scenario in args[\"scenarios\"]\n",
    "    ]\n",
    "else:\n",
    "    datasets = []\n",
    "\n",
    "if 'ssp585_mri-esm2-0' in datasets:\n",
    "    print(\"Removed invalid dataset: ssp585_mri-esm2-0\", )\n",
    "    datasets.remove('ssp585_mri-esm2-0')\n",
    "\n",
    "if args[\"use_era5\"]:\n",
    "    datasets.insert(0, \"era5\")\n",
    "\n",
    "# print args used for training and store them\n",
    "config_path = os.path.join(out_dir, f\"{args['experiment_name']}_config.yaml\")\n",
    "\n",
    "with open(config_path, \"w\") as f:\n",
    "    yaml.dump(args, f)\n",
    "\n",
    "print(\"...TRAINING ARGUMENTS...\")\n",
    "print(yaml.dump(args, sort_keys=False, default_flow_style=False))\n",
    "print(\"........................\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Optimizing Lasso---\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "(MSE, MSSS, MAE, R2)\n",
      "Lasso: [0.34551097 0.24223207], [0.03920573 0.20096413], [0.41284299 0.34409765], [-0.09951184 -0.00375262]\n",
      "All models optimized\n",
      "---Testing (MSE, MSSS, MAE, R2)---\n",
      "Climatology: [0.33257771 0.23989816], [0. 0.], [0.40452103 0.34467835], [-0.05835462  0.00591855]\n",
      "Optimize_ML_Lasso: [0.34551097 0.24223207], [-0.03888793 -0.00972875], [0.41284299 0.34409765], [-0.09951184 -0.00375262]\n"
     ]
    }
   ],
   "source": [
    "X_train_all, y_train_all = [], []\n",
    "X_val_all, y_val_all = [], []\n",
    "\n",
    "for dataset in datasets:\n",
    "    df = pd.read_csv(f\"{preprocessing_path}/{dataset}.csv\", index_col=0, engine='python', encoding='utf-8').reset_index()\n",
    "\n",
    "    # Custom or predefined train/val split function\n",
    "    X_train, y_train, X_val, y_val = train_test_split(df, include_years=False, gap_years = args[\"train_val_YearsGap\"])\n",
    "\n",
    "    # Accumulate\n",
    "    X_train_all.append(X_train)\n",
    "    y_train_all.append(y_train)\n",
    "    X_val_all.append(X_val)\n",
    "    y_val_all.append(y_val)\n",
    "\n",
    "# Concatenate all at once (more efficient than repeated appends)\n",
    "X_train_merged = pd.concat(X_train_all, ignore_index=True)\n",
    "y_train_merged = pd.concat(y_train_all, ignore_index=True)\n",
    "X_val_merged = pd.concat(X_val_all, ignore_index=True)\n",
    "y_val_merged = pd.concat(y_val_all, ignore_index=True)\n",
    "\n",
    "# Train models\n",
    "models = optimize_and_train_model(X_train_merged, y_train_merged, X_val_merged, y_val_merged, models_dict, search_spaces, time_aggr=\"annual\", target_lag_prefix = \"_DJF\", store_models = True, out_path = out_dir, store_training = True, experiment_name=args[\"experiment_name\"])\n",
    "evaluate_models(X_val_merged, y_val_merged, models, store_validation=True, out_path = out_dir, experiment_name=args[\"experiment_name\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
