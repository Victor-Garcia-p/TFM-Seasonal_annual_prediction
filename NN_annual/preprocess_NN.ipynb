{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script prepares input datasets (seasonal maps, annual maps, ENSO index) for training and evaluation of climate prediction models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order produced is variable_index=0=pr_anom; variable_index_1=pr_anom\n",
    "\n",
    "from warnings import warn\n",
    "import sys\n",
    "import xarray as xr\n",
    "\n",
    "sys.path.append('/home/vgarcia/notebooks')\n",
    "from preprocessing_functions import *\n",
    "from experiments_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input file\n",
    "out_preprocess_basepath = \"/data/dl20-data/climate_operational/Victor_data/preprocessed_datasets_NN_new\"\n",
    "test_mode = False\n",
    "\n",
    "# Define datasets to process\n",
    "use_era5 = True\n",
    "process_test_dataset = True\n",
    "lag_index = True                # use Niño3.4 from the previous month\n",
    "cmip6_models = []\n",
    "scenarios = []\n",
    "\n",
    "cmip6_models = [\n",
    "    \"access-cm2\",\n",
    "    \"cmcc-esm2\",\n",
    "    \"inm-cm4-8\",\n",
    "    \"inm-cm5-0\",\n",
    "    \"miroc-es2l\",\n",
    "    \"mpi-esm1-2-lr\",\n",
    "    \"mri-esm2-0\",\n",
    "    \"noresm2-mm\"\n",
    "]\n",
    "\n",
    "scenarios = [\"historical\", \"ssp126\", \"ssp245\", \"ssp585\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added era5\n"
     ]
    }
   ],
   "source": [
    "# note: ssp585_mri-esm2-0 does not exist, it will be removed\n",
    "datasets = []\n",
    "datasets.extend([\n",
    "    f\"{scenario}_{model}\"\n",
    "    for model in cmip6_models\n",
    "    for scenario in scenarios\n",
    "])\n",
    "\n",
    "# list all datasets to process\n",
    "\n",
    "if 'ssp585_mri-esm2-0' in datasets:\n",
    "    print(\"Removed invalid dataset: ssp585_mri-esm2-0\", )\n",
    "    datasets.remove('ssp585_mri-esm2-0')\n",
    "\n",
    "if use_era5:\n",
    "    print(\"Added era5\")\n",
    "    datasets.insert(0, \"era5\")\n",
    "\n",
    "if process_test_dataset:\n",
    "    print(\"Added test dataset\")\n",
    "    datasets.insert(0, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_seasonal_data(filtered_dict, variables=[\"pr_anom\", \"rx90p_anom\", \"txx_anom\"]):\n",
    "    season_order = [\"DJF\", \"MAM\", \"JJA\", \"SON\"]\n",
    "    all_arrays = []\n",
    "    season_labels = []\n",
    "    year_labels = []\n",
    "\n",
    "    for season_idx, season in enumerate(season_order):\n",
    "        for var_idx, variable in enumerate(variables):\n",
    "            key = f\"{variable}_{season}\"\n",
    "            da = filtered_dict[key].sortby(\"time\")  # ensure time is sorted\n",
    "\n",
    "            # Get years and validate\n",
    "            years = da['time'].dt.year.values\n",
    "            unique_years, counts = np.unique(years, return_counts=True)\n",
    "            if not np.all(counts == 1):\n",
    "                raise ValueError(f\"Each year must appear exactly once in {key}.\")\n",
    "\n",
    "            all_arrays.append(da)\n",
    "            season_labels.extend([season_idx * len(variables) + var_idx] * len(years))\n",
    "            year_labels.extend(years)\n",
    "\n",
    "    # Concatenate all seasonal data arrays into one along time\n",
    "    da_merged = xr.concat(all_arrays, dim='time')\n",
    "\n",
    "    # Create a MultiIndex for (year, season_index)\n",
    "    multiindex = pd.MultiIndex.from_arrays([year_labels, season_labels], names=['year', 'season_index'])\n",
    "\n",
    "    # Assign MultiIndex to time\n",
    "    da_merged.coords['time'] = multiindex\n",
    "\n",
    "    # Unstack to (year, season, lat, lon)\n",
    "    da_unstacked = da_merged.unstack('time')\n",
    "\n",
    "    # Transpose to ensure correct dimension order\n",
    "    da_final = da_unstacked.transpose('year', 'season_index', 'lat', 'lon')\n",
    "\n",
    "    return da_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_annual_data(dataset, variables=[\"pr_anom\", \"rx90p_anom\"]):\n",
    "    # Merge selected variables into one long DataArray along time\n",
    "    merged_list = [dataset[var] for var in variables]\n",
    "    da_merged = xr.concat(merged_list, dim='time').sortby('time')\n",
    "\n",
    "    # Extract years and ensure each appears twice\n",
    "    years = da_merged['time'].dt.year.values\n",
    "    unique_years, counts = np.unique(years, return_counts=True)\n",
    "    if not np.all(counts == len(variables)):\n",
    "        raise ValueError(\"Each year must appear exactly twice in the time dimension.\")\n",
    "\n",
    "    n_years = len(unique_years)\n",
    "\n",
    "    # Create indexing arrays\n",
    "    year_index = np.repeat(unique_years, len(variables))\n",
    "    variable_index = np.tile([0, 1], n_years)\n",
    "\n",
    "    # Create a MultiIndex for stacking\n",
    "    multiindex = pd.MultiIndex.from_arrays([year_index, variable_index], names=['year', 'variable_index'])\n",
    "\n",
    "    # Assign the MultiIndex to the time dimension\n",
    "    da_merged.coords['time'] = multiindex\n",
    "\n",
    "    # Unstack to shape (year, time_index, lat, lon)\n",
    "    da_unstacked = da_merged.unstack('time')\n",
    "\n",
    "    # Rearrange dimensions\n",
    "    da_final = da_unstacked.transpose('year', 'variable_index', 'lat', 'lon')\n",
    "\n",
    "    return da_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing: era5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_109026/1816542680.py:22: FutureWarning: the `pandas.MultiIndex` object(s) passed as 'time' coordinate(s) or data variable(s) will no longer be implicitly promoted and wrapped into multiple indexed coordinates in the future (i.e., one coordinate for each multi-index level + one dimension coordinate). If you want to keep this behavior, you need to first wrap it explicitly using `mindex_coords = xarray.Coordinates.from_pandas_multiindex(mindex_obj, 'dim')` and pass it as coordinates, e.g., `xarray.Dataset(coords=mindex_coords)`, `dataset.assign_coords(mindex_coords)` or `dataarray.assign_coords(mindex_coords)`.\n",
      "  da_merged.coords['time'] = multiindex\n",
      "/tmp/ipykernel_109026/49106058.py:29: FutureWarning: the `pandas.MultiIndex` object(s) passed as 'time' coordinate(s) or data variable(s) will no longer be implicitly promoted and wrapped into multiple indexed coordinates in the future (i.e., one coordinate for each multi-index level + one dimension coordinate). If you want to keep this behavior, you need to first wrap it explicitly using `mindex_coords = xarray.Coordinates.from_pandas_multiindex(mindex_obj, 'dim')` and pass it as coordinates, e.g., `xarray.Dataset(coords=mindex_coords)`, `dataset.assign_coords(mindex_coords)` or `dataarray.assign_coords(mindex_coords)`.\n",
      "  da_merged.coords['time'] = multiindex\n",
      "/home/vgarcia/notebooks/preprocessing_functions.py:572: FutureWarning: the `pandas.MultiIndex` object(s) passed as 'time' coordinate(s) or data variable(s) will no longer be implicitly promoted and wrapped into multiple indexed coordinates in the future (i.e., one coordinate for each multi-index level + one dimension coordinate). If you want to keep this behavior, you need to first wrap it explicitly using `mindex_coords = xarray.Coordinates.from_pandas_multiindex(mindex_obj, 'dim')` and pass it as coordinates, e.g., `xarray.Dataset(coords=mindex_coords)`, `dataset.assign_coords(mindex_coords)` or `dataarray.assign_coords(mindex_coords)`.\n",
      "  da.coords['time'] = multiindex\n",
      "/tmp/ipykernel_109026/4159110299.py:54: SerializationWarning: variable None has data in the form of a dask array with dtype=object, which means it is being loaded into memory to determine a data type that can be safely stored on disk. To avoid this, coerce this variable to a fixed-size dtype with astype() before saving it.\n",
      "  season_da.to_zarr(out_path + \"season_da.zarr\", mode=\"w\")\n",
      "/home/vgarcia/miniconda3/envs/myenv/lib/python3.11/site-packages/dask/array/reductions.py:324: RuntimeWarning: All-NaN slice encountered\n",
      "  return np.nanmax(x_chunk, axis=axis, keepdims=keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "era5 stored on /data/dl20-data/climate_operational/Victor_data/preprocessed_datasets_NN_new/era5/\n"
     ]
    }
   ],
   "source": [
    "for dataset in datasets:\n",
    "    print(\"Preprocessing:\", dataset)\n",
    "    # make the out path if it does not exist\n",
    "    out_path = out_preprocess_basepath + f\"/{dataset}/\"\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "\n",
    "    ## Load and preprocess indicators maps\n",
    "    indicators_dict = calculate_indicators(indicators = [\"rx90p\", \"pr\", \"txx\"], dataset = dataset, test=test_mode)\n",
    "    lagged_dict = annual_preprocessing(indicators_dict)\n",
    "\n",
    "    annual_da = merge_annual_data(lagged_dict[\"annual_ds\"])\n",
    "    start_year = lagged_dict[\"annual_ds\"].time.min().dt.year.values + 1\n",
    "    end_year = lagged_dict[\"annual_ds\"].time.max().dt.year.values\n",
    "    annual_da = annual_da.sel(year = slice(start_year, end_year))\n",
    "\n",
    "    filtered_dict = lagged_dict.copy()\n",
    "    del filtered_dict[\"annual_ds\"]\n",
    "    season_da = merge_seasonal_data(filtered_dict).sel(year = slice(start_year, end_year))\n",
    "\n",
    "    # Load Niño3.4 indexes\n",
    "    if dataset == \"era5\":\n",
    "        index_path = \"/data/dl20-data/climate_operational/Victor_data/climate_index/Nino3.4/NASA-Nino3.4.txt\"\n",
    "    elif dataset == \"test\":\n",
    "        index_path = \"/data/dl20-data/climate_operational/Victor_data/climate_index/Nino3.4/NASA-Nino3.4-HadISST.txt\"\n",
    "    else:\n",
    "        index_path = f\"/data/dl20-data/climate_operational/Victor_data/climate_index/Nino3.4/{dataset}_Nino3.4.txt\"\n",
    "\n",
    "    if lag_index:\n",
    "        index_da = load_lagged_index(index_path = index_path, time_range = [str(start_year), str(end_year)])\n",
    "    else:\n",
    "        print(\"Warning: ENSO index is not lagged\")\n",
    "        index_da = load_lagged_index(index_path = index_path, time_range = [str(start_year), str(end_year)], lag_one_year=False)\n",
    "\n",
    "    # Ensure they have the same number of years and the desired dimensions\n",
    "    index_max_year = index_da.year.max()\n",
    "    if index_max_year < end_year:\n",
    "        season_da = season_da.sel(year=slice(None, index_max_year))\n",
    "        annual_da = annual_da.sel(year=slice(None, index_max_year))\n",
    "\n",
    "    if season_da.shape[0] != annual_da.shape[0] or index_da.shape[0] != annual_da.shape[0]:\n",
    "        raise IndexError(\"Not all Xarray have the same number of years\")\n",
    "    elif season_da.shape[1] != 12 or annual_da.shape[1] != 2:\n",
    "        raise IndexError(\"Season or annual does not have the desired dimensions\")\n",
    "    \n",
    "    # store all datasets\n",
    "    # store all datasets\n",
    "    season_da.name = \"season_maps\"\n",
    "    season_da = season_da.chunk({\n",
    "        \"year\": -1,      \n",
    "        \"lat\": 1,                \n",
    "        \"lon\": 1,\n",
    "        \"season_index\": 1\n",
    "    })\n",
    "    season_da.to_zarr(out_path + \"season_da.zarr\", mode=\"w\")\n",
    "\n",
    "    annual_da.name = \"annual_maps\"\n",
    "    annual_da = annual_da.chunk({\n",
    "        \"year\": -1,      \n",
    "        \"lat\": 1,                \n",
    "        \"lon\": 1,\n",
    "        \"variable_index\": 1\n",
    "    })\n",
    "    annual_da.to_zarr(out_path + \"annual_da.zarr\", mode=\"w\")\n",
    "\n",
    "    index_da.name = \"Nino3.4\"\n",
    "    index_da = index_da.chunk({\n",
    "        \"year\": -1,                    \n",
    "        \"season\": 1\n",
    "    })\n",
    "\n",
    "    if lag_index:\n",
    "        index_da.to_zarr(out_path + \"index_da.zarr\", mode=\"w\")\n",
    "    else:\n",
    "        index_da.to_zarr(out_path + \"index_da_NotLagged.zarr\", mode=\"w\")\n",
    "    print(dataset, \"stored on\", out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
