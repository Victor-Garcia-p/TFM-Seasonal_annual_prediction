{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script trains a deep learning model (e.g., UNet variants) for annual climate prediction using ENSO-related datasets. It loads training configurations from a YAML file, prepares datasets (CMIP6 and ERA5), splits data with a validation gap, and trains the selected model using a custom loss. It supports warm starts, early stopping, periodic checkpoints, and logs performance metrics (e.g., loss, R¬≤, MAE). Best model weights and training logs are saved for further evaluation or reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from warnings import warn\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "\n",
    "from models_NN import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YAML with training settup\n",
    "with open('/home/vgarcia/NN/config_train.yml', 'r') as file:\n",
    "    args = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check inputs\n",
    "model_dict = {\"SmallUNet\" : SmallUNet(),\n",
    "              \"UNet\": UNet(),\n",
    "              \"UNetAdjusted\": UNetAdjusted(),\n",
    "              \"UNetSkip\": UNet_Skip()}\n",
    "\n",
    "checkpoint_dir = f\"/home/vgarcia/experiments/NN_annual/{args['experiment_name']}\"\n",
    "csv_path = os.path.join(checkpoint_dir, f\"{args['experiment_name']}_training_metrics.csv\")\n",
    "os.makedirs(checkpoint_dir, exist_ok=args[\"overwrite_experiment\"])\n",
    "\n",
    "pretrained_model_path = checkpoint_dir + f\"/{args['experiment_name']}_{args['model_name']}_best.pt\"\n",
    "\n",
    "# ensure models and parameters exist\n",
    "if args['model_name'] not in model_dict:\n",
    "    raise NotImplementedError\n",
    "\n",
    "# list all datasets to process\n",
    "if \"cmip6\" and \"scenarios\" in args:\n",
    "    datasets = [\n",
    "        f\"{scenario}_{model}\"\n",
    "        for model in args[\"cmip6_models\"]\n",
    "        for scenario in args[\"scenarios\"]\n",
    "    ]\n",
    "else:\n",
    "    datasets = []\n",
    "\n",
    "if 'ssp585_mri-esm2-0' in datasets:\n",
    "    print(\"Removed invalid dataset: ssp585_mri-esm2-0\", )\n",
    "    datasets.remove('ssp585_mri-esm2-0')\n",
    "\n",
    "if args[\"use_era5\"]:\n",
    "    datasets.insert(0, \"era5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print args used for training and store them\n",
    "config_path = os.path.join(checkpoint_dir, f\"{args['experiment_name']}_{args['model_name']}_config.yaml\")\n",
    "with open(config_path, \"w\") as f:\n",
    "    yaml.dump(args, f)\n",
    "\n",
    "print(\"...TRAINING ARGUMENTS...\")\n",
    "print(yaml.dump(args, sort_keys=False, default_flow_style=False))\n",
    "print(\"........................\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_season = []\n",
    "all_annual = []\n",
    "all_index = []\n",
    "\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "year_offset = 0  # To track global year index across merged datasets\n",
    "\n",
    "for dataset in datasets:\n",
    "    # Load preprocessed datasets\n",
    "    season_da = xr.open_zarr(args['preprocessing_path'] + f\"/{dataset}/season_da.zarr\")\n",
    "    season_da = season_da[list(season_da.data_vars)[0]]\n",
    "\n",
    "    annual_da = xr.open_zarr(args['preprocessing_path'] + f\"/{dataset}/annual_da.zarr\")\n",
    "    annual_da = annual_da[list(annual_da.data_vars)[0]]\n",
    "\n",
    "    if args[\"Lag_index\"] == False:\n",
    "        warn(\"Loading ENSO index without lag\")\n",
    "        index_da = xr.open_zarr(args['preprocessing_path'] + f\"/{dataset}/index_da_NotLagged.zarr\")\n",
    "    else:\n",
    "        index_da = xr.open_zarr(args['preprocessing_path'] + f\"/{dataset}/index_da.zarr\")\n",
    "    index_da = index_da[list(index_da.data_vars)[0]]\n",
    "\n",
    "    # select some years to test\n",
    "    if args[\"test_mode\"] == True:\n",
    "        print(\"Test mode\")\n",
    "        season_da = season_da.isel(year = slice(0, 5))\n",
    "        annual_da = annual_da.isel(year = slice(0, 5))\n",
    "        index_da = index_da.isel(year = slice(0, 5))\n",
    "\n",
    "    all_season.append(season_da)\n",
    "    all_annual.append(annual_da)\n",
    "    all_index.append(index_da)\n",
    "\n",
    "    # split indexes for train and validation\n",
    "    n_years = season_da.sizes['year']\n",
    "    \n",
    "    # Per-dataset split with 5-year gap\n",
    "    n_train = int(0.7 * n_years)\n",
    "\n",
    "    if n_train + args['train_val_YearsGap'] >= n_years:\n",
    "        raise ValueError(f\"Dataset {dataset} is too small to leave a 5-year gap after training.\")\n",
    "\n",
    "    train_idx = list(range(year_offset, year_offset + n_train))\n",
    "    val_idx = list(range(year_offset + n_train + args['train_val_YearsGap'], year_offset + n_years))\n",
    "\n",
    "    train_indices.extend(train_idx)\n",
    "    val_indices.extend(val_idx)\n",
    "\n",
    "    year_offset += n_years\n",
    "\n",
    "# concatenate all datasets\n",
    "merged_season = xr.concat(all_season, dim='year')\n",
    "merged_annual = xr.concat(all_annual, dim='year')\n",
    "merged_index = xr.concat(all_index, dim='year')\n",
    "\n",
    "train_dataset = XarrayENSODataset(merged_season, merged_index, merged_annual, train_indices)\n",
    "val_dataset = XarrayENSODataset(merged_season, merged_index, merged_annual, val_indices)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(123)\n",
    "train_loader = DataLoader(train_dataset, batch_size=args[\"batch_size\"], num_workers = args[\"num_workers\"], shuffle = True, generator = g)\n",
    "val_loader = DataLoader(val_dataset, batch_size=args[\"batch_size\"], num_workers = args[\"num_workers\"])\n",
    "\n",
    "# Setup model, optimizer, loss\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model_dict[args[\"model_name\"]].to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = MaskedMSELoss()\n",
    "\n",
    "# Load checkpoint if warm start\n",
    "if args[\"warm_start\"] and os.path.exists(pretrained_model_path):\n",
    "    checkpoint = torch.load(pretrained_model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    print(f\"‚úÖ Loaded model from {pretrained_model_path}, trained until epoch {checkpoint['epoch']}\")\n",
    "    best_score = checkpoint[\"score\"]\n",
    "else:\n",
    "    best_score = -np.inf\n",
    "\n",
    "start_time = time()\n",
    "limit_epoch = args[\"early_stop\"] if args[\"early_stop\"] else None\n",
    "metrics_log = []\n",
    "\n",
    "for epoch in range(args[\"num_epochs\"]):\n",
    "    if args[\"early_stop\"] and limit_epoch is not None and limit_epoch < 0:\n",
    "        print(f\"‚õî Early stopping triggered for '{args['experiment_name']}'\\n\")\n",
    "        break\n",
    "\n",
    "    train_loss = train_model(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, score, r2_raw, mae_raw = evaluate_model(model, val_loader, criterion, device)\n",
    "\n",
    "    print(f\"\\n Epoch {epoch + 1}/f{args['num_epochs']}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f} | Score: {score:.4f}\")\n",
    "    print(f\"  R¬≤: {r2_raw} | MAE: {mae_raw}\")\n",
    "\n",
    "    # store metrics\n",
    "    metrics_log.append({\n",
    "        \"experiment\": args['experiment_name'],\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"score\": score,\n",
    "        \"r2_pr\": r2_raw[0],\n",
    "        \"r2_rx90p\": r2_raw[1],\n",
    "        \"mae_pr\": mae_raw[0],\n",
    "        \"mae_rx90p\": mae_raw[1],\n",
    "        \"timestamp\": datetime.now()})\n",
    "\n",
    "    metrics = [train_loss, val_loss, score] + list(r2_raw) + list(mae_raw)\n",
    "    if any(np.isnan(val) for val in metrics):\n",
    "        warn(\"NaN detected ‚Äî stopping training early.\")\n",
    "        break\n",
    "\n",
    "    # Save best model\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_model_path = os.path.join(checkpoint_dir, f\"{args['experiment_name']}_{args['model_name']}_best.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'score': score,\n",
    "            'val_loss': val_loss,\n",
    "            'experiment_name': args[\"experiment_name\"],\n",
    "            'date': datetime.now()\n",
    "        }, best_model_path)\n",
    "        print(f\"‚úÖ Best model saved ‚Üí {best_model_path}\")\n",
    "\n",
    "        # Reset early stop counter\n",
    "        if args[\"early_stop\"]:\n",
    "            limit_epoch = args[\"early_stop\"]\n",
    "\n",
    "    else:\n",
    "        # Decrease early stop counter\n",
    "        if args[\"early_stop\"]:\n",
    "            limit_epoch -= 1\n",
    "\n",
    "    # Periodic checkpoint\n",
    "    if (epoch + 1) % args[\"save_every_n_epochs\"] == 0:\n",
    "        periodic_path = os.path.join(checkpoint_dir, f\"{args['experiment_name']}_{args['model_name']}_{epoch + 1:03d}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'score': score,\n",
    "            'val_loss': val_loss,\n",
    "            'experiment_name': args[\"experiment_name\"],\n",
    "            'date': datetime.now(),\n",
    "            'model_name': args[\"model_name\"]\n",
    "        }, periodic_path)\n",
    "        print(f\"üóÇÔ∏è Periodic checkpoint saved ‚Üí {periodic_path}\")\n",
    "\n",
    "        # Save metrics up to current epoch\n",
    "        metrics_df = pd.DataFrame(metrics_log)\n",
    "        metrics_checkpoint_path = os.path.join(checkpoint_dir, f\"{args['experiment_name']}_{args['model_name']}_metrics.csv\")\n",
    "        metrics_df.to_csv(metrics_checkpoint_path, index=False)\n",
    "        print(f\"üìä Metrics saved ‚Üí {metrics_checkpoint_path}\")\n",
    "\n",
    "    warm_start = True  # use previous model in next round\n",
    "\n",
    "print(f\"\\n‚úÖ Training completed for dataset '{args['experiment_name']}' in {((time() - start_time) / 60):.2f} minutes.\\n\")\n",
    "\n",
    "# Final save of all metrics\n",
    "final_metrics_path = os.path.join(checkpoint_dir, f\"{args['experiment_name']}_{args['model_name']}_metrics.csv\")\n",
    "final_metrics_df = pd.DataFrame(metrics_log)\n",
    "final_metrics_df.to_csv(final_metrics_path, mode='a', header=not os.path.exists(final_metrics_path), index=False)\n",
    "print(f\"üìä Final metrics saved ‚Üí {final_metrics_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
